# Spark packages for Kafka, Delta Lake, and other connectors
spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0,org.apache.hadoop:hadoop-aws:3.3.6,org.apache.spark:spark-streaming-kafka-0-10_2.13:4.0.0,io.delta:delta-spark_2.13:3.2.0

# Iceberg configuration (commented out - using Delta Lake instead)
# spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.5.2,org.apache.hadoop:hadoop-aws:3.3.6,org.apache.spark:spark-streaming-kafka-0-10_2.13:4.0.0,io.delta:delta-spark_2.13:3.2.0
# spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension
# spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog
# spark.sql.catalog.spark_catalog.type=hive
# spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog
# spark.sql.catalog.local.type=hadoop
# spark.sql.catalog.local.warehouse=s3a://warehouse/

# Delta Lake extensions
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# S3/MinIO configuration
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=admin
spark.hadoop.fs.s3a.secret.key=password123
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.fast.upload=true

# Kafka configuration for Structured Streaming
spark.sql.streaming.kafka.useDeprecatedOffsetFetching=false

# Structured Streaming settings
spark.sql.streaming.checkpointLocation.root=s3a://checkpoints/
spark.sql.streaming.forceDeleteTempCheckpointLocation=true
spark.sql.streaming.minBatchesToRetain=10
spark.sql.streaming.maxBatchesToRetainInMemory=5

# Performance settings 
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.localShuffleReader.enabled=true
spark.sql.adaptive.optimizer.excludedRules=
spark.serializer=org.apache.spark.serializer.KryoSerializer

# Streaming-specific memory settings
spark.streaming.receiver.maxRate=1000
spark.streaming.kafka.maxRatePerPartition=1000
spark.streaming.backpressure.enabled=true

# Delta Lake support
spark.sql.catalog.delta=org.apache.spark.sql.delta.catalog.DeltaCatalog

# Hive Metastore
spark.sql.catalogImplementation=hive
spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083

# # Elasticsearch connector
# spark.es.nodes=elasticsearch
# spark.es.port=9200
# spark.es.nodes.wan.only=true
